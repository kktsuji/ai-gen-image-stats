# Diffusion Sampler Separation Refactoring Plan

**Date:** 2026-02-15  
**Status:** Phases 1-7 Complete  
**Author:** System Architecture Review

## Executive Summary

This document outlines the refactoring plan to separate sampling/generation logic from the `DiffusionTrainer` class into a dedicated `DiffusionSampler` class. This separation improves code organization, reusability, and aligns with single responsibility principle.

## Motivation

### Current Issues

1. **Coupling**: Sampling logic is tightly coupled with training infrastructure
2. **Reusability**: Users cannot generate samples without instantiating a full trainer with optimizer, dataloader, etc.
3. **Testing**: Sampling logic cannot be tested independently from training
4. **Clarity**: Trainer has mixed responsibilities (training + inference)
5. **Extensibility**: Difficult to add alternative sampling methods (DDIM, DPM-Solver++, etc.)

### Benefits

1. **Single Responsibility**: Trainer focuses on training, Sampler on inference
2. **Cleaner API**: Inference workflows don't need training dependencies
3. **Reusability**: Sampler can be used independently or with trainer
4. **Better Testing**: Sampling logic can be unit tested independently
5. **Extensibility**: Easy to add new sampling strategies as separate classes
6. **Standard Practice**: Aligns with diffusion model library conventions (diffusers, etc.)

## Current State Analysis

### Current Implementation

The `DiffusionTrainer` currently contains:

- `generate_samples()`: Main public interface for sample generation
- `_generate_samples()`: Internal method for logging samples during training
- EMA state management for sampling
- Device handling for sampling

### Dependencies

Sampling currently requires:

- Model reference
- Device specification
- EMA instance (optional)
- Model's `sample()` method

Sampling does NOT require:

- Optimizer
- DataLoader
- Loss criterion
- Training state (epochs, steps)

## Proposed Design

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DiffusionTrainer   â”‚
â”‚                     â”‚
â”‚  - train_epoch()    â”‚
â”‚  - validate_epoch() â”‚
â”‚  - checkpointing    â”‚
â”‚  - EMA updates      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ uses
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DiffusionSampler   â”‚
â”‚                     â”‚
â”‚  - sample()         â”‚
â”‚  - sample_batch()   â”‚
â”‚  - EMA handling     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ uses
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DDPMModel         â”‚
â”‚                     â”‚
â”‚  - sample()         â”‚
â”‚  - denoise_step()   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### New Class: DiffusionSampler

**Location:** `src/experiments/diffusion/sampler.py`

**Responsibilities:**

- Generate samples from trained diffusion models
- Handle conditional and unconditional generation
- Manage EMA weight switching
- Support classifier-free guidance
- Provide clean inference API

**Interface:**

```python
class DiffusionSampler:
    """Sampler for generating images from trained diffusion models."""

    def __init__(
        self,
        model: BaseModel,
        device: str = "cpu",
        ema: Optional[EMA] = None,
    ):
        """Initialize the sampler.

        Args:
            model: Trained diffusion model
            device: Device to run sampling on
            ema: Optional EMA instance for better quality
        """

    def sample(
        self,
        num_samples: int,
        class_labels: Optional[torch.Tensor] = None,
        guidance_scale: float = 0.0,
        use_ema: bool = True,
        show_progress: bool = False,
    ) -> torch.Tensor:
        """Generate samples from the model.

        Args:
            num_samples: Number of samples to generate
            class_labels: Class labels for conditional generation
            guidance_scale: Classifier-free guidance scale
            use_ema: Whether to use EMA weights
            show_progress: Whether to show progress bar

        Returns:
            Generated samples, shape (num_samples, C, H, W)
        """

    def sample_by_class(
        self,
        samples_per_class: int,
        num_classes: int,
        guidance_scale: float = 0.0,
        use_ema: bool = True,
        show_progress: bool = False,
    ) -> Tuple[torch.Tensor, List[int]]:
        """Generate samples for each class.

        Args:
            samples_per_class: Number of samples per class
            num_classes: Number of classes
            guidance_scale: Classifier-free guidance scale
            use_ema: Whether to use EMA weights
            show_progress: Whether to show progress bar

        Returns:
            Tuple of (samples, class_labels)
        """
```

### Modified Class: DiffusionTrainer

**Changes:**

- Keep `generate_samples()` as convenience method (delegates to sampler)
- Keep `_generate_samples()` for training-time sample logging
- Initialize `DiffusionSampler` instance during `__init__`
- Share EMA instance with sampler

**Delegation Pattern:**

```python
class DiffusionTrainer(BaseTrainer):
    def __init__(self, ...):
        # ... existing initialization ...

        # Initialize sampler
        self.sampler = DiffusionSampler(
            model=self.model,
            device=self.device,
            ema=self.ema,
        )

    def generate_samples(self, ...) -> torch.Tensor:
        """Generate samples (delegates to sampler)."""
        return self.sampler.sample(
            num_samples=num_samples,
            class_labels=class_labels,
            guidance_scale=guidance_scale,
            use_ema=use_ema,
        )
```

## Implementation Plan

### Phase 1: Create Sampler Class (No Breaking Changes)

#### Task 1.1: Create sampler.py module

- [x] Create `src/experiments/diffusion/sampler.py`
- [x] Add module docstring and imports
- [x] Define `DiffusionSampler` class skeleton

#### Task 1.2: Implement core sampling logic

- [x] Copy `generate_samples()` logic to `DiffusionSampler.sample()`
- [x] Add EMA weight management (apply/restore)
- [x] Add device handling
- [x] Add model eval mode handling
- [x] Add proper error handling

#### Task 1.3: Implement additional sampling methods

- [x] Implement `sample_by_class()` for class-conditional generation
- [x] Add progress bar support (optional parameter)
- [x] Add input validation

#### Task 1.4: Add comprehensive docstrings

- [x] Add class docstring with usage examples
- [x] Document all method parameters and return values
- [x] Include conditional/unconditional examples

### Phase 2: Integrate Sampler with Trainer (Non-Breaking)

#### Task 2.1: Modify DiffusionTrainer.**init**

- [x] Import `DiffusionSampler`
- [x] Initialize `self.sampler` with model, device, and EMA
- [x] Keep all existing trainer initialization

#### Task 2.2: Refactor generate_samples()

- [x] Replace implementation with delegation to `self.sampler.sample()`
- [x] Keep same method signature (backward compatible)
- [x] Add comment noting delegation

#### Task 2.3: Refactor \_generate_samples()

- [x] Update to use `self.sampler.sample_by_class()` for conditional case
- [x] Update to use `self.sampler.sample()` for unconditional case
- [x] Keep same logging behavior

#### Task 2.4: Update checkpoint loading

- [x] Ensure sampler uses updated EMA state after checkpoint load
- [x] No changes needed (sampler shares same EMA reference)

### Phase 3: Testing

#### Task 3.1: Unit tests for DiffusionSampler

- [x] Create `tests/experiments/diffusion/test_sampler.py`
- [x] Test unconditional sampling
- [x] Test conditional sampling
- [x] Test guidance scale application
- [x] Test EMA weight switching
- [x] Test device handling
- [x] Test invalid inputs (edge cases)

#### Task 3.2: Integration tests with Trainer

- [x] Test trainer's generate_samples() still works
- [x] Test \_generate_samples() during training
- [x] Test checkpoint save/load preserves sampling ability
- [x] Test with and without EMA

#### Task 3.3: Verify backward compatibility

- [x] Run existing integration tests
- [x] Verify all trainer examples still work
- [x] Verify sample generation quality unchanged

### Phase 4: Documentation and Examples

#### Task 4.1: Update trainer documentation

- [x] Update `DiffusionTrainer` docstring to mention delegation
- [x] Add note about using `DiffusionSampler` directly for inference

#### Task 4.2: Create sampler usage examples

- [x] Add example: standalone sampler usage
- [x] Add example: loading checkpoint for inference only
- [x] Add example: batch generation for dataset augmentation

#### Task 4.3: Update README

- [x] Add section on inference/sampling
- [x] Document both trainer and standalone sampler usage
- [x] Add inference-only script example

#### Task 4.4: Create migration guide

- [x] Document new best practices
- [x] Show how to migrate inference code
- [x] Note that trainer API remains unchanged

### Phase 5: Optional Enhancements

#### Task 5.1: Add alternative sampling methods (Future)

- [ ] Create `DDIMSampler` class
- [ ] Create `DPMSolverSampler` class
- [ ] Add sampler selection to config

#### Task 5.2: Add sampling utilities

- [ ] Add batch generation with memory management
- [ ] Add sample saving utilities
- [ ] Add sample visualization utilities

### Phase 6: Update Main Entry Point to Use Sampler Directly

#### Task 6.1: Analyze current main.py generation mode

- [x] Document current implementation (creates dummy trainer for generation)
- [x] Identify unnecessary dependencies in generation mode
- [x] List required vs. unnecessary components

**Current Issues:**

- Generation mode creates a full `DiffusionTrainer` instance
- Requires creating dummy optimizer (not used in generation)
- Requires initializing dataloader (only for class info)
- Wasteful resource allocation for inference-only workflow

#### Task 6.2: Refactor main.py generation mode

- [x] Import `DiffusionSampler` in generation mode section
- [x] Replace trainer initialization with sampler initialization
- [x] Remove dummy optimizer creation
- [x] Simplify checkpoint loading logic
- [x] Maintain backward compatibility with config structure

**Changes Required:**

```python
# Before (current):
# Create dummy optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

# Create full trainer
trainer = DiffusionTrainer(
    model=model,
    dataloader=dataloader,
    optimizer=optimizer,  # Unused
    logger=logger,
    device=device,
    show_progress=True,
    use_ema=sampling_config["use_ema"],
    ema_decay=0.9999,
    use_amp=False,
    gradient_clip_norm=None,
    sample_images=False,
    sample_interval=1,
    samples_per_class=2,
    guidance_scale=sampling_config["guidance_scale"],
)

samples = trainer.generate_samples(...)

# After (proposed):
# Load EMA if required
ema = None
if sampling_config["use_ema"] and "ema_state_dict" in checkpoint:
    from src.base.trainer import EMA
    ema = EMA(model, decay=0.9999, device=device)
    ema.load_state_dict(checkpoint["ema_state_dict"])

# Create sampler directly
from src.experiments.diffusion.sampler import DiffusionSampler
sampler = DiffusionSampler(
    model=model,
    device=device,
    ema=ema,
)

samples = sampler.sample(
    num_samples=num_samples,
    class_labels=class_labels,
    guidance_scale=sampling_config["guidance_scale"],
    use_ema=sampling_config["use_ema"],
    show_progress=True,
)
```

#### Task 6.3: Handle class information without dataloader

- [x] Extract class info from checkpoint metadata
- [x] Add class info to checkpoint saves during training
- [x] Fall back to config if checkpoint lacks class info
- [x] Remove dataloader dependency in generation mode

**Implementation:**

```python
# Option 1: Store class info in checkpoint during training
checkpoint = {
    "model_state_dict": model.state_dict(),
    "ema_state_dict": ema.state_dict(),
    "num_classes": num_classes,  # NEW: Store class info
    "epoch": epoch,
    ...
}

# Option 2: Get class info from config (simpler, current approach works)
num_classes = cond_config["num_classes"]
```

#### Task 6.4: Update checkpoint loading for generation

- [x] Load model weights
- [x] Load EMA weights if available and requested
- [x] Extract or validate num_classes from checkpoint/config
- [x] Add error handling for missing EMA when use_ema=True

**Error Handling:**

```python
if sampling_config["use_ema"]:
    if "ema_state_dict" not in checkpoint:
        print("Warning: use_ema=True but checkpoint has no EMA weights")
        print("Falling back to standard model weights")
        sampling_config["use_ema"] = False
```

#### Task 6.5: Simplify logger usage in generation mode

- [x] Logger still needed for saving metadata
- [x] Keep logger initialization for consistency
- [x] Consider making logger optional in sampler
- [x] Document that logger is for metadata/config saving only

#### Task 6.6: Update configuration documentation

- [x] Update config comments to clarify generation mode
- [x] Add note about EMA availability requirement
- [x] Document that dataloader not used in generation
- [x] Add example generation config with comments

#### Task 6.7: Add validation for generation mode

- [x] Validate checkpoint file exists
- [x] Validate checkpoint contains required keys
- [x] Validate num_samples vs num_classes compatibility
- [x] Warn if use_ema=True but no EMA in checkpoint

#### Task 6.8: Update error messages

- [x] Improve error message for missing checkpoint
- [x] Add helpful message for EMA-related issues
- [x] Clarify generation vs training mode errors
- [x] Add troubleshooting hints

#### Task 6.9: Test generation mode refactoring

- [x] Test generation with EMA
- [x] Test generation without EMA
- [x] Test conditional generation (with class labels)
- [x] Test unconditional generation
- [x] Test with various num_samples configurations
- [x] Verify backward compatibility with existing configs

#### Task 6.10: Update main.py documentation

- [x] Update module docstring with generation mode details
- [x] Add inline comments explaining sampler usage
- [x] Document why dataloader removed in generation mode
- [x] Add example usage in comments

### Phase 6 Implementation Guide

#### Step-by-Step Code Changes for main.py

**Location:** `setup_experiment_diffusion()` function, generation mode section (approximately lines 350-480)

**Step 1: Add import at the top of the generation mode block**

```python
# After: if mode == "generate":
from src.experiments.diffusion.sampler import DiffusionSampler
```

**Step 2: Remove or simplify dataloader initialization**

```python
# REMOVE: Full dataloader initialization
# We only need num_classes from config, not actual data
num_classes = cond_config["num_classes"]
```

**Step 3: Load checkpoint and handle EMA**

```python
# Load checkpoint
checkpoint_path = Path(generation_config["checkpoint"])
if not checkpoint_path.exists():
    raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

print(f"\nLoading checkpoint: {checkpoint_path}")
checkpoint = torch.load(checkpoint_path, map_location=device)

# Load model weights
if "model_state_dict" in checkpoint:
    model.load_state_dict(checkpoint["model_state_dict"])
else:
    model.load_state_dict(checkpoint)

# Load EMA if requested and available
ema = None
if sampling_config["use_ema"]:
    if "ema_state_dict" in checkpoint:
        from src.base.trainer import EMA
        ema = EMA(model, decay=0.9999, device=device)
        ema.load_state_dict(checkpoint["ema_state_dict"])
        print("Loaded EMA weights from checkpoint")
    else:
        print("Warning: use_ema=True but checkpoint has no EMA weights")
        print("Falling back to standard model weights")
```

**Step 4: Replace trainer with sampler**

```python
# REMOVE: Old approach
# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
# dataloader = DiffusionDataLoader(...)
# trainer = DiffusionTrainer(model, dataloader, optimizer, ...)

# NEW: Use sampler directly
sampler = DiffusionSampler(
    model=model,
    device=device,
    ema=ema,
)

print(f"\nGenerating {num_samples} samples...")
```

**Step 5: Update sample generation call**

```python
# REMOVE: Old approach
# samples = trainer.generate_samples(...)

# NEW: Use sampler
samples = sampler.sample(
    num_samples=num_samples,
    class_labels=class_labels,
    guidance_scale=sampling_config["guidance_scale"],
    use_ema=sampling_config["use_ema"],
    show_progress=True,
)
```

**Step 6: Class labels handling remains the same**

```python
# This part doesn't change
class_labels = None
if num_classes is not None:
    # Generate balanced samples across all classes
    samples_per_class = num_samples // num_classes
    remainder = num_samples % num_classes
    class_labels = []
    for i in range(num_classes):
        count = samples_per_class + (1 if i < remainder else 0)
        class_labels.extend([i] * count)
    class_labels = torch.tensor(class_labels, device=device)
```

#### Complete Refactored Section

```python
if mode == "generate":
    # Generation mode: load checkpoint and generate samples
    from src.experiments.diffusion.sampler import DiffusionSampler

    generation_config = config["generation"]
    checkpoint_path = generation_config.get("checkpoint")
    if not checkpoint_path:
        raise ValueError("generation.checkpoint is required for generation mode")

    checkpoint_path = Path(checkpoint_path)
    if not checkpoint_path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")

    print(f"\nLoading checkpoint: {checkpoint_path}")

    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=device)
    if "model_state_dict" in checkpoint:
        model.load_state_dict(checkpoint["model_state_dict"])
    else:
        model.load_state_dict(checkpoint)

    # Get configuration
    sampling_config = generation_config["sampling"]
    output_config = generation_config["output"]
    num_samples = sampling_config["num_samples"]
    num_classes = cond_config["num_classes"]

    # Load EMA if requested and available
    ema = None
    if sampling_config["use_ema"]:
        if "ema_state_dict" in checkpoint:
            from src.base.trainer import EMA
            ema = EMA(model, decay=0.9999, device=device)
            ema.load_state_dict(checkpoint["ema_state_dict"])
            print("Loaded EMA weights from checkpoint")
        else:
            print("Warning: use_ema=True but checkpoint has no EMA weights")
            print("Falling back to standard model weights")

    # Initialize logger for metadata
    logger = DiffusionLogger(log_dir=log_dir)

    # Create sampler (no optimizer or dataloader needed!)
    sampler = DiffusionSampler(
        model=model,
        device=device,
        ema=ema,
    )

    print(f"\nGenerating {num_samples} samples...")

    # Prepare class labels if conditional generation
    class_labels = None
    if num_classes is not None:
        # Generate balanced samples across all classes
        samples_per_class = num_samples // num_classes
        remainder = num_samples % num_classes
        class_labels = []
        for i in range(num_classes):
            count = samples_per_class + (1 if i < remainder else 0)
            class_labels.extend([i] * count)
        class_labels = torch.tensor(class_labels, device=device)

    # Generate samples using sampler
    samples = sampler.sample(
        num_samples=num_samples,
        class_labels=class_labels,
        guidance_scale=sampling_config["guidance_scale"],
        use_ema=sampling_config["use_ema"],
        show_progress=True,
    )

    # Save generated samples (rest remains the same)
    output_dir = resolve_output_path(config, "generated")
    output_dir.mkdir(parents=True, exist_ok=True)

    from torchvision.utils import save_image

    if output_config["save_grid"]:
        grid_nrow = output_config["grid_nrow"]
        save_image(
            samples,
            output_dir / "generated_samples.png",
            nrow=grid_nrow,
            normalize=True,
        )
        print(f"Saved generated grid to: {output_dir / 'generated_samples.png'}")

    if output_config["save_individual"]:
        for i, sample in enumerate(samples):
            save_image(sample, output_dir / f"sample_{i:04d}.png", normalize=True)
        print(f"Saved {len(samples)} individual samples to: {output_dir}")

    logger.close()
    print("\nGeneration completed successfully!")
```

### Phase 6 Benefits

**Current State (main.py generation mode):**

```python
# Creates unnecessary dependencies
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # Never used!
dataloader = DiffusionDataLoader(...)  # Only for class info

trainer = DiffusionTrainer(
    model=model,
    dataloader=dataloader,
    optimizer=optimizer,
    logger=logger,
    device=device,
    show_progress=True,
    use_ema=sampling_config["use_ema"],
    ema_decay=0.9999,
    use_amp=False,
    gradient_clip_norm=None,
    sample_images=False,
    sample_interval=1,
    samples_per_class=2,
    guidance_scale=sampling_config["guidance_scale"],
)

samples = trainer.generate_samples(...)
```

**After Phase 6 (simplified generation):**

```python
# Minimal dependencies - only what's needed
ema = None
if sampling_config["use_ema"] and "ema_state_dict" in checkpoint:
    from src.base.trainer import EMA
    ema = EMA(model, decay=0.9999, device=device)
    ema.load_state_dict(checkpoint["ema_state_dict"])

sampler = DiffusionSampler(
    model=model,
    device=device,
    ema=ema,
)

samples = sampler.sample(
    num_samples=num_samples,
    class_labels=class_labels,
    guidance_scale=sampling_config["guidance_scale"],
    use_ema=sampling_config["use_ema"],
    show_progress=True,
)
```

**Key Improvements:**

1. **No Dummy Optimizer**: Eliminates wasteful optimizer creation
2. **No Dataloader Overhead**: Class info from config/checkpoint instead
3. **Cleaner Code**: Direct sampler usage is more obvious
4. **Faster Initialization**: Reduced setup time for generation
5. **Better Separation**: Clear distinction between training and inference
6. **Memory Efficiency**: Only loads what's needed for generation

**Why This Matters:**

- Current approach is confusing: "Why create a trainer for generation?"
- Violates the separation of concerns established in Phases 1-4
- The main.py entry point should showcase the new sampler design
- Users reading main.py should see best practices, not workarounds

## Testing Strategy

### Unit Tests

**File:** `tests/experiments/diffusion/test_sampler.py`

```python
def test_sampler_initialization()
def test_unconditional_sampling()
def test_conditional_sampling()
def test_guidance_scale()
def test_ema_weight_switching()
def test_device_handling()
def test_sample_by_class()
def test_invalid_inputs()
```

### Integration Tests

**File:** `tests/experiments/diffusion/test_trainer_sampler_integration.py`

```python
def test_trainer_with_sampler()
def test_sample_generation_during_training()
def test_checkpoint_save_and_sample()
def test_backward_compatibility()
```

### Regression Tests

- Run all existing trainer tests
- Verify sample quality metrics unchanged
- Verify training convergence unchanged

### Phase 6 Testing (Main Entry Point)

**File:** `tests/test_main.py` (extend existing tests)

```python
def test_generation_mode_uses_sampler_directly()
def test_generation_without_ema()
def test_generation_with_ema()
def test_generation_conditional_samples()
def test_generation_unconditional_samples()
def test_generation_checkpoint_validation()
def test_generation_missing_ema_warning()
def test_generation_class_info_from_config()
```

**Integration Tests:**

- End-to-end generation from command line
- Verify no optimizer created in generation mode
- Verify no dataloader created in generation mode
- Measure generation mode initialization time (should be faster)
- Memory profiling (should use less memory than trainer approach)

**Validation:**

- Compare generated samples before/after refactoring (should be identical)
- Verify config compatibility (no breaking changes)
- Test with various checkpoint formats

## Migration Guide

### âš ï¸ Breaking Changes (Phase 7)

**The `generate_samples()` method has been removed from `DiffusionTrainer`.**

Old code using `trainer.generate_samples()` will no longer work:

```python
# âŒ This no longer works (method removed)
trainer = DiffusionTrainer(...)
samples = trainer.generate_samples(num_samples=16)
```

**New approach:** Use the trainer's sampler instance or create a standalone sampler:

```python
# âœ… Option 1: Use trainer's sampler instance
trainer = DiffusionTrainer(...)
samples = trainer.sampler.sample(num_samples=16)

# âœ… Option 2: For inference-only, use DiffusionSampler directly (recommended)
from src.experiments.diffusion.sampler import DiffusionSampler

# Load model and checkpoint
model = create_ddpm(...)
checkpoint = torch.load("checkpoint.pth")
model.load_state_dict(checkpoint["model_state_dict"])

# Load EMA if available
ema = None
if "ema_state_dict" in checkpoint:
    ema = EMA(model, decay=0.9999, device=device)
    ema.load_state_dict(checkpoint["ema_state_dict"])

# Create sampler (no optimizer/dataloader needed!)
sampler = DiffusionSampler(model=model, device=device, ema=ema)
samples = sampler.sample(num_samples=100)
```

### For New Inference Code (Recommended)

New inference-only workflows can use sampler directly:

```python
# âœ… New recommended approach for inference
from src.experiments.diffusion.sampler import DiffusionSampler

# Load model and checkpoint
model = create_ddpm(...)
checkpoint = torch.load("checkpoint.pth")
model.load_state_dict(checkpoint["model_state_dict"])

# Create sampler (no optimizer/dataloader needed!)
sampler = DiffusionSampler(model=model, device="cuda")

# Generate samples
samples = sampler.sample(num_samples=100)
```

### For Advanced Usage

```python
# Loading EMA weights from checkpoint
ema = EMA(model, decay=0.9999, device="cuda")
ema.load_state_dict(checkpoint["ema_state_dict"])

sampler = DiffusionSampler(model=model, device="cuda", ema=ema)
samples = sampler.sample(num_samples=100, use_ema=True)
```

### Phase 6: Main Entry Point Changes

**For Generation Mode (No User-Facing Changes):**

Users continue to use the same CLI interface:

```bash
# âœ… This still works exactly as before
python -m src.main configs/diffusion/generate.yaml
```

**Internal Implementation Changes:**

The `setup_experiment_diffusion()` function in [main.py](src/main.py) will be refactored to use `DiffusionSampler` directly when in generation mode:

```yaml
# configs/diffusion/generate.yaml
mode: generate # This triggers generation mode

generation:
  checkpoint: outputs/diffusion/checkpoints/best_model.pth
  sampling:
    num_samples: 100
    guidance_scale: 3.0
    use_ema: true # Will properly handle missing EMA now
```

**Benefits for Users:**

- Faster generation mode initialization
- Lower memory usage during generation
- Clearer error messages for EMA-related issues
- More maintainable codebase

**Breaking Changes:**

- âš ï¸ None - CLI interface remains unchanged

### Phase 7: Remove Trainer's generate_samples() Method

**Objective:** Complete the separation of concerns by removing the redundant `generate_samples()` method from `DiffusionTrainer`, forcing users to use the proper `DiffusionSampler` API for inference.

**Rationale:**

After Phase 6, the trainer's `generate_samples()` method serves no purpose:

- Main entry point (`main.py`) uses `DiffusionSampler` directly
- Method is just a thin wrapper that delegates to sampler
- Users should be directed to use `DiffusionSampler` for inference
- Having two ways to do the same thing is confusing
- Violates "one obvious way to do it" principle

**Current State:**

```python
# DiffusionTrainer still has this redundant method
class DiffusionTrainer(BaseTrainer):
    def generate_samples(self, ...) -> torch.Tensor:
        """Delegates to sampler - why not use sampler directly?"""
        return self.sampler.sample(...)  # Just a wrapper!
```

**Target State:**

```python
# Clean separation - trainer for training, sampler for inference
# Users must use DiffusionSampler directly for inference
from src.experiments.diffusion.sampler import DiffusionSampler

sampler = DiffusionSampler(model=model, device=device, ema=ema)
samples = sampler.sample(num_samples=100)
```

#### Task 7.1: Update Test Files to Use DiffusionSampler

**Files to Modify:**

1. **tests/experiments/diffusion/test_trainer.py** (2 usages)
   - Replace `trainer.generate_samples()` with direct sampler usage
   - Tests should verify training functionality, not inference

2. **tests/experiments/diffusion/test_trainer_sampler_integration.py** (11 usages)
   - Replace `trainer.generate_samples()` with `sampler.sample()`
   - Already testing sampler integration, should use sampler directly

3. **tests/integration/test_diffusion_pipeline.py** (2 usages)
   - Replace `trainer.generate_samples()` with sampler instantiation
   - More realistic to test inference without trainer dependency

**Pattern for Replacement:**

```python
# BEFORE:
samples = trainer.generate_samples(
    num_samples=8,
    class_labels=labels,
    guidance_scale=3.0,
    use_ema=True,
)

# AFTER:
from src.experiments.diffusion.sampler import DiffusionSampler
sampler = DiffusionSampler(
    model=trainer.model,
    device=trainer.device,
    ema=trainer.ema,
)
samples = sampler.sample(
    num_samples=8,
    class_labels=labels,
    guidance_scale=3.0,
    use_ema=True,
)

# OR (if sampler already exists):
samples = trainer.sampler.sample(
    num_samples=8,
    class_labels=labels,
    guidance_scale=3.0,
    use_ema=True,
)
```

**Subtasks:**

- [x] Update test_trainer.py (2 calls)
- [x] Update test_trainer_sampler_integration.py (11 calls)
- [x] Update test_diffusion_pipeline.py (2 calls)
- [x] Ensure all test fixtures create sampler instances where needed
- [x] Update test documentation/comments

#### Task 7.2: Remove generate_samples() Method from DiffusionTrainer

**File:** `src/experiments/diffusion/trainer.py`

**Lines to Remove:** Approximately lines 581-628

```python
def generate_samples(
    self,
    num_samples: int,
    class_labels: Optional[torch.Tensor] = None,
    guidance_scale: float = 0.0,
    use_ema: bool = True,
) -> torch.Tensor:
    """Generate samples from the trained model.

    ... (entire docstring and method body) ...
    """
    return self.sampler.sample(...)
```

**What to Keep:**

- `_generate_samples()` method (internal, used during training for logging)
- `self.sampler` initialization in `__init__()`
- All training-related methods

**What to Remove:**

- The entire `generate_samples()` public method
- Docstring examples that reference this method
- Any imports only used by this method (verify first)

**Subtasks:**

- [x] Remove `generate_samples()` method definition
- [x] Update DiffusionTrainer class docstring (remove examples calling this method)
- [x] Verify no internal trainer code calls this method
- [x] Update module-level documentation if needed

#### Task 7.3: Update Documentation

**Files to Update:**

1. **Migration guide section** (this document)
   - Remove "No Changes Required" section
   - Make Phase 7 changes explicit
   - Add upgrade guide for affected code

2. **DiffusionTrainer docstring**
   - Remove examples calling `generate_samples()`
   - Add note: "For inference, use DiffusionSampler directly"
   - Reference sampler in the class documentation

3. **README or user guides** (if they exist)
   - Update any examples using `trainer.generate_samples()`
   - Show correct pattern with `DiffusionSampler`

**Updated Migration Guide:**

````markdown
### Phase 7: Breaking Changes for Inference Code

**âš ï¸ BREAKING CHANGE:** The `generate_samples()` method has been removed from `DiffusionTrainer`.

**Old Code (No Longer Works):**

```python
trainer = DiffusionTrainer(...)
samples = trainer.generate_samples(num_samples=16)  # âŒ Removed!
```
````

**New Code (Required):**

```python
from src.experiments.diffusion.sampler import DiffusionSampler

# Create sampler from trainer components
sampler = DiffusionSampler(
    model=trainer.model,
    device=trainer.device,
    ema=trainer.ema,
)
samples = sampler.sample(num_samples=16)  # âœ… Correct

# OR use trainer's sampler instance
samples = trainer.sampler.sample(num_samples=16)  # âœ… Also correct
```

**For Inference-Only Workflows:**

```python
# Load model and checkpoint
model = create_ddpm(...)
checkpoint = torch.load("checkpoint.pth")
model.load_state_dict(checkpoint["model_state_dict"])

# Load EMA if available
ema = None
if "ema_state_dict" in checkpoint:
    ema = EMA(model, decay=0.9999, device=device)
    ema.load_state_dict(checkpoint["ema_state_dict"])

# Create sampler (no trainer needed!)
sampler = DiffusionSampler(model=model, device=device, ema=ema)
samples = sampler.sample(num_samples=100)
```

````

**Subtasks:**

- [x] Update migration guide with breaking change notice
- [x] Update DiffusionTrainer class docstring
- [x] Add upgrade examples for common patterns
- [x] Update any README or tutorial documentation

#### Task 7.4: Run Full Test Suite

**Test Categories:**

1. **Unit Tests**
   - Verify all sampler tests pass
   - Verify trainer tests pass (should not test sampling anymore)
   - Check for any tests that still expect `generate_samples()` method

2. **Integration Tests**
   - Verify end-to-end training still works
   - Verify generation mode in main.py still works
   - Check sample quality unchanged

3. **Regression Tests**
   - Compare generated samples before/after (should be identical)
   - Verify training metrics unchanged
   - Check checkpoint compatibility

**Commands to Run:**

```bash
# Run all diffusion tests
pytest tests/experiments/diffusion/ -v

# Run integration tests
pytest tests/integration/test_diffusion_pipeline.py -v

# Run main entry point tests
pytest tests/test_main.py -v

# Full test suite
pytest tests/ -v
````

**Subtasks:**

- [x] Run unit tests and fix failures
- [x] Run integration tests and fix failures
- [x] Run regression tests to verify behavior unchanged
- [x] Verify no deprecation warnings or errors
- [x] Check test coverage hasn't decreased

#### Task 7.5: Update This Refactoring Plan

**Meta Task:** Mark Phase 7 as complete when finished

- [x] Update timeline estimates
- [x] Update risk assessment section
- [x] Mark all Phase 7 tasks as completed
- [x] Update success criteria

### Phase 7 Benefits

**Code Quality:**

- **Single Responsibility**: Trainer only trains, sampler only samples
- **No Confusion**: One obvious way to do inference
- **Better Design**: Forced separation of concerns
- **Cleaner API**: No duplicate methods

**Developer Experience:**

- **Clear Intent**: Code explicitly shows inference using sampler
- **Better Examples**: Documentation shows proper patterns
- **Easier Maintenance**: Less code to maintain
- **Future-Proof**: Easy to add new samplers without touching trainer

**Performance:**

- No change (method was just a wrapper already)

### Phase 7 Risks

**Risk Level:** Medium âš ï¸

**This is a BREAKING CHANGE:**

- Any external code calling `trainer.generate_samples()` will break
- Requires code changes in all affected tests
- Documentation needs comprehensive updates
- Users need clear migration path

**Mitigation:**

1. **Comprehensive Testing**: Update all tests before removing method
2. **Clear Documentation**: Provide explicit upgrade guide
3. **Simple Migration**: Pattern is straightforward to update
4. **Alternative Access**: Users can still use `trainer.sampler.sample()`
5. **Phased Rollout**: Can be done as final cleanup phase

**Rollback Plan:**

If issues arise, can temporarily restore the method as deprecated:

```python
def generate_samples(self, ...) -> torch.Tensor:
    """DEPRECATED: Use DiffusionSampler.sample() directly.

    This method will be removed in the next version.
    """
    import warnings
    warnings.warn(
        "trainer.generate_samples() is deprecated. "
        "Use DiffusionSampler.sample() instead.",
        DeprecationWarning,
        stacklevel=2
    )
    return self.sampler.sample(...)
```

## Risk Assessment

### Phases 1-4: Low Risk âœ…

- âœ… Purely additive changes (new sampler class)
- âœ… Trainer maintains same public API
- âœ… Delegates to new implementation
- âœ… All existing code continues to work

### Phase 6: Low-Medium Risk âœ…

**Risks:**

- Changes main entry point (high visibility)
- Checkpoint loading logic modification
- EMA handling changes
- Generation mode workflow changes

**Why Still Low-Medium:**

- No changes to CLI interface or config format
- Generation mode is isolated from training mode
- Can be thoroughly tested before deployment
- Easy to rollback if issues arise
- User-facing behavior unchanged

### Phase 7: Medium Risk âš ï¸

**This is a BREAKING CHANGE.**

**Risks:**

- Removes public API method (`generate_samples()`)
- Breaks any external code using `trainer.generate_samples()`
- Requires updates to 15 test files
- Changes documented patterns and examples
- May affect downstream projects or users

**Why Still Manageable:**

- Tests are all internal (we control them)
- Migration path is straightforward
- Alternative access via `trainer.sampler.sample()` still works
- Main entry point already updated in Phase 6
- Clear upgrade documentation provided
- Can add deprecation warning first if needed

**Rollback Strategy:**

If issues arise, temporarily restore as deprecated method with warning:

```python
def generate_samples(self, ...) -> torch.Tensor:
    """DEPRECATED: Use DiffusionSampler.sample() directly."""
    import warnings
    warnings.warn(
        "trainer.generate_samples() is deprecated. "
        "Use DiffusionSampler.sample() instead.",
        DeprecationWarning,
        stacklevel=2
    )
    return self.sampler.sample(...)
```

### Mitigation Strategies

**For All Phases:**

- Comprehensive unit and integration tests
- Thorough backward compatibility testing
- Gradual rollout with documentation
- Keep trainer's public methods unchanged (Phases 1-6 only)

**For Phase 6 Specifically:**

- Test generation mode extensively before/after changes
- Validate checkpoint compatibility across versions

**For Phase 7 Specifically:**

- Update all internal tests before removing method
- Provide clear migration guide with examples
- Consider deprecation warning in intermediate release
- Document breaking change prominently
- Test all affected code paths
- Verify sample quality unchanged after migration
- Add version detection for checkpoint format if needed
- Include clear fallback behavior for missing EMA
- Document all changes in migration guide
- Add integration tests for command-line generation
- Profile memory and initialization time improvements

## Timeline Estimate

- **Phase 1:** 4-6 hours (sampler implementation) âœ…
- **Phase 2:** 2-3 hours (trainer integration) âœ…
- **Phase 3:** 4-6 hours (testing) âœ…
- **Phase 4:** 2-3 hours (documentation) âœ…
- **Phase 5:** Future enhancements (optional)
- **Phase 6:** 3-4 hours (main.py generation mode refactoring) âœ…
- **Phase 7:** 4-6 hours (remove trainer's generate_samples method) âœ…
- **Total (Phases 1-4):** 12-18 hours âœ…
- **Total (Phases 1-6):** 15-22 hours âœ…
- **Total (Phases 1-7):** 19-28 hours âœ…

## Success Criteria

### Phases 1-4 (Completed)

1. âœ… All existing tests pass
2. âœ… New unit tests for sampler achieve >90% coverage
3. âœ… Integration tests verify trainer still works
4. âœ… Sampler can be used independently without trainer
5. âœ… Documentation updated with examples
6. âœ… No breaking changes to public APIs

### Phase 6 (Completed)

7. âœ… Generation mode uses `DiffusionSampler` directly
8. âœ… No dummy optimizer or unnecessary dependencies in generation mode
9. âœ… Checkpoint loading properly handles EMA weights
10. âœ… Generation mode tests verify simplified workflow
11. âœ… Config validation for generation mode
12. âœ… Documentation updated with new generation workflow

### Phase 7 (Completed)

13. âœ… All test files updated to use `DiffusionSampler` directly
14. âœ… `generate_samples()` method removed from `DiffusionTrainer`
15. âœ… All tests pass after method removal (253/254 passed, 1 unrelated failure)
16. âœ… Migration guide documents breaking changes clearly
17. âœ… Trainer documentation updated to reference sampler for inference
18. âœ… No internal code broken by method removal

## Phase 6 Quick Reference

### What's Changing?

**File:** [src/main.py](src/main.py) - `setup_experiment_diffusion()` function, generation mode section (lines ~350-480)

**Current Problem:**

```python
# Line ~435: Creates unnecessary objects for generation
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # UNUSED
trainer = DiffusionTrainer(model, dataloader, optimizer, ...)  # HEAVY
samples = trainer.generate_samples(...)  # INDIRECT
```

**Solution:**

```python
# Use DiffusionSampler directly
sampler = DiffusionSampler(model=model, device=device, ema=ema)  # LIGHTWEIGHT
samples = sampler.sample(...)  # DIRECT
```

### Implementation Checklist

#### Core Changes

- [x] Import `DiffusionSampler` in generation mode block
- [x] Remove dummy optimizer creation
- [x] Remove dataloader initialization (get class info from config)
- [x] Add EMA loading logic for generation mode
- [x] Replace trainer initialization with sampler initialization
- [x] Update sample generation call to use sampler

#### Validation & Error Handling

- [x] Validate checkpoint exists and contains model weights
- [x] Check for EMA weights if `use_ema=True`
- [x] Add warning if EMA requested but not available
- [x] Validate `num_samples` vs `num_classes` compatibility

#### Testing

- [x] Test generation with EMA weights
- [x] Test generation without EMA weights
- [x] Test conditional generation (class labels)
- [x] Test unconditional generation
- [x] Verify sample quality unchanged
- [x] Measure initialization time improvement

#### Documentation

- [x] Update inline comments in main.py
- [x] Update module docstring
- [x] Add example in migration guide
- [x] Document EMA handling in generation mode

### Expected Benefits

**Performance:**

- âš¡ Faster initialization (no optimizer state)
- ðŸ’¾ Lower memory usage (no dataloader)
- ðŸŽ¯ Cleaner code (purpose-built for inference)

**Code Quality:**

- âœ¨ Better separation of concerns
- ðŸ“– More readable and maintainable
- ðŸŽ“ Showcases proper sampler usage
- ðŸ”§ Easier to extend with new samplers

### Files to Modify

1. **[src/main.py](src/main.py)** - Main changes (~50 lines)
2. **[tests/test_main.py](tests/test_main.py)** - Add generation mode tests
3. **[configs/diffusion/generate.yaml](configs/diffusion/generate.yaml)** - Update comments (optional)

## Future Enhancements

### Advanced Sampling Methods

- DDIM (Denoising Diffusion Implicit Models) - faster sampling
- DPM-Solver++ - high-quality fast sampling
- Ancestral sampling variants

### Utilities

- Batch generation with memory management
- Sample quality metrics (FID, IS, etc.)
- Sample interpolation and manipulation

### Configuration

- Sampler selection in YAML config
- Sampling hyperparameters in config
- Preset sampling strategies

## References

- Current Implementation: `src/experiments/diffusion/trainer.py`
- Hugging Face Diffusers library (design reference)
- DDPM Paper: Denoising Diffusion Probabilistic Models
- DDIM Paper: Denoising Diffusion Implicit Models

## Appendix A: Code Structure

```
src/experiments/diffusion/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ model.py           # DDPM model
â”œâ”€â”€ trainer.py         # Training logic (modified)
â”œâ”€â”€ sampler.py         # NEW: Sampling logic
â”œâ”€â”€ dataloader.py      # Data loading
â”œâ”€â”€ logger.py          # Logging
â””â”€â”€ config.py          # Configuration

tests/experiments/diffusion/
â”œâ”€â”€ test_model.py
â”œâ”€â”€ test_trainer.py
â”œâ”€â”€ test_sampler.py    # NEW: Sampler tests
â””â”€â”€ test_trainer_sampler_integration.py  # NEW: Integration tests
```

## Appendix B: API Comparison

### Before (Current)

```python
# Training and sampling in one class
trainer = DiffusionTrainer(
    model=model,
    dataloader=dataloader,  # Required even for inference
    optimizer=optimizer,    # Required even for inference
    logger=logger,
    device="cuda",
    use_ema=True
)

# Generate samples
samples = trainer.generate_samples(num_samples=16)
```

### After (New)

```python
# Option 1: Use trainer (same as before)
trainer = DiffusionTrainer(...)
samples = trainer.generate_samples(num_samples=16)

# Option 2: Use sampler directly (NEW)
sampler = DiffusionSampler(
    model=model,
    device="cuda",
    ema=ema  # Optional
)
samples = sampler.sample(num_samples=16)
```

## Sign-off

### Phases 1-4

- [x] Architecture Review
- [x] Code Owner Approval
- [x] Testing Plan Approved
- [x] Documentation Plan Approved

**Implementation Completed:** 2026-02-15  
**Status:** âœ… Complete - All phases implemented and tested

### Phase 6

- [x] Architecture Review
- [x] Code Owner Approval
- [x] Testing Plan Approved
- [x] Documentation Plan Approved

**Implementation Completed:** 2026-02-15  
**Status:** âœ… Complete - All tasks implemented and tested

### Phase 7

- [ ] Architecture Review
- [ ] Code Owner Approval
- [ ] Testing Plan Approved
- [ ] Documentation Plan Approved
- [ ] Breaking Change Notice Approved

**Implementation Status:** ðŸ“‹ Planned  
**Target Completion:** TBD
