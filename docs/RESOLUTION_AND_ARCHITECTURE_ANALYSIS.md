# Resolution and Architecture Analysis for DDPM

**Date:** November 23, 2025  
**Issue:** Generated images show jaggy/non-smooth boundaries  
**Current Setup:** 40√ó40 resolution, 3-stage U-Net

---

## Problem Analysis

### Current Architecture Limitations

**Resolution:** 40√ó40 pixels (1,600 total pixels)
- **Bottleneck:** 10√ó10 = 100 pixels (too small for detail preservation)
- **Architecture:** 3 downsampling stages with `channel_multipliers=(1, 2, 4)`
- **Parameters:** ~15-20M
- **Issue:** Jaggy boundaries in generated images

**Resolution Flow:**
```
40√ó40 ‚Üí 20√ó20 ‚Üí 10√ó10 (bottleneck) ‚Üí 20√ó20 ‚Üí 40√ó40
  64     128      256                  128      64
```

### Root Causes of Boundary Issues

1. **Extremely Low Resolution (Primary)**
   - 40√ó40 = only 1,600 pixels total
   - Each pixel represents significant portion of image
   - Smooth transitions nearly impossible with so few pixels

2. **ConvTranspose2d Upsampling (Secondary)**
   - Current implementation uses `nn.ConvTranspose2d` with stride=2
   - Known to cause checkerboard artifacts and jagged edges
   - Better alternative: bilinear interpolation + Conv2d

3. **Limited Dataset Size**
   - Normal: 437 images, Abnormal: 85 images (522 total)
   - Very small for deep learning, but workable with augmentation

4. **Shallow Architecture**
   - Only 3 downsampling stages
   - Limited multi-scale feature learning
   - Insufficient receptive field for context understanding

---

## Latent Diffusion Model (LDM) Consideration

### Would LDM Help?

**Answer: NO, not recommended for this case**

**Reasons:**
1. Resolution too low (40√ó40) to benefit from latent compression
2. Dataset too small (522 images) to train a good VAE
3. Added complexity doesn't justify marginal benefits
4. LDM designed for high-res images (512√ó512+) where pixel-space is expensive
5. Would likely introduce additional VAE artifacts

**When LDM Would Help:**
- Images at 256√ó256 or higher resolution
- Significantly more training data (10k+ images)
- Computational cost of pixel-space diffusion is prohibitive

---

## Recommended Solutions

### Priority 1: Increase Resolution ‚≠ê MOST IMPACTFUL

**Recommendation: Use 128√ó128 or 256√ó256**

**Benefits:**
- 128√ó128 = 10x more pixels than 40√ó40
- 256√ó256 = 40x more pixels than 40√ó40
- Enables smooth gradient transitions
- Allows deeper architectures

**Expected Impact:** 70-80% reduction in jaggedness

### Priority 2: Fix Upsampling Method ‚≠ê QUICK WIN

**Current (problematic):**
```python
nn.ConvTranspose2d(channels, channels, kernel_size=4, stride=2, padding=1)
```

**Recommended:**
```python
nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
nn.Conv2d(channels, channels, kernel_size=3, padding=1)
```

**Benefits:**
- Eliminates checkerboard artifacts
- Smoother upsampling
- Easy to implement

### Priority 3: Data Augmentation

**Recommendations:**
```python
transforms.Compose([
    transforms.Resize((140, 140)),
    transforms.RandomCrop((128, 128)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=10),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
```

**Expected Impact:** 10-15% additional improvement

### Priority 4: Architecture Improvements

**For 128√ó128:**
```python
channel_multipliers = (1, 2, 2, 4, 8)  # 5 stages
base_channels = 64
num_res_blocks = 2
use_attention = (False, False, False, True, True)
```

**For 256√ó256:**
```python
channel_multipliers = (1, 2, 2, 4, 4, 8)  # 6 stages
base_channels = 64
num_res_blocks = 2
use_attention = (False, False, False, False, True, True)
```

**Expected Impact:** 5-10% improvement

---

## Architecture Modifications for Higher Resolution

### Do You Need to Modify `ddpm.py`?

**Answer: NO - The code is already flexible!**

Your `ddpm.py` already supports arbitrary `channel_multipliers` lengths. Just pass different parameters when creating the model.

### Architecture Comparison

#### Current (40√ó40, 3 stages)
```
Input: 40√ó40
‚îú‚îÄ Down1: 40√ó40 ‚Üí 40√ó40 (64 channels, no downsample)
‚îú‚îÄ Down2: 40√ó40 ‚Üí 20√ó20 (64 ‚Üí 128, downsample)
‚îú‚îÄ Down3: 20√ó20 ‚Üí 10√ó10 (128 ‚Üí 256, downsample)
‚îú‚îÄ Middle: 10√ó10 (256 channels) ‚Üê Bottleneck too small!
‚îú‚îÄ Up1: 10√ó10 ‚Üí 20√ó20 (256)
‚îú‚îÄ Up2: 20√ó20 ‚Üí 40√ó40 (128)
‚îú‚îÄ Up3: 40√ó40 ‚Üí 40√ó40 (64, no upsample)
Output: 40√ó40

Parameters: ~15-20M
Bottleneck: 10√ó10 = 100 pixels ‚ùå
```

#### Recommended for 128√ó128 (5 stages)
```
Input: 128√ó128
‚îú‚îÄ Down1: 128√ó128 ‚Üí 128√ó128 (64 channels)
‚îú‚îÄ Down2: 128√ó128 ‚Üí 64√ó64 (64 ‚Üí 128)
‚îú‚îÄ Down3: 64√ó64 ‚Üí 32√ó32 (128 ‚Üí 128)
‚îú‚îÄ Down4: 32√ó32 ‚Üí 16√ó16 (128 ‚Üí 256)
‚îú‚îÄ Down5: 16√ó16 ‚Üí 8√ó8 (256 ‚Üí 512)
‚îú‚îÄ Middle: 8√ó8 (512 channels) ‚Üê Good bottleneck! ‚úì
‚îú‚îÄ Up1-5: Reverse process
Output: 128√ó128

Parameters: ~40-50M (2.5-3x more)
Bottleneck: 8√ó8 = 64 pixels ‚úì
Attention: At 16√ó16 and 8√ó8 ‚úì
```

#### Optional for 256√ó256 (6 stages)
```
Input: 256√ó256
‚îú‚îÄ 6 downsampling stages
‚îú‚îÄ Middle: 8√ó8 (512 channels)
‚îú‚îÄ 6 upsampling stages
Output: 256√ó256

Parameters: ~50-60M
Bottleneck: 8√ó8 = 64 pixels ‚úì
```

### Why Deeper Architecture Matters

**Key Insight: More Parameters = Better Feature Learning**

#### Benefits of Deeper Networks (5-6 stages vs 3):

1. **Multi-Scale Feature Hierarchy**
   - Shallow (3 stages): Basic edges ‚Üí Simple shapes ‚Üí Object parts
   - Deep (5-6 stages): Edges ‚Üí Textures ‚Üí Patterns ‚Üí Structures ‚Üí Compositions ‚Üí Global context

2. **More Parameters = More Capacity**
   - Current: ~15-20M parameters
   - Deeper: ~40-50M parameters (2.5-3x more)
   - More expressiveness for subtle abnormality patterns
   - Better boundary smoothness
   - Better generalization

3. **Larger Receptive Field**
   - Understands surrounding context
   - Learns nuanced, smooth boundaries
   - Captures both fine details AND global structure

4. **Better Information Flow**
   - Bottleneck at 8√ó8 or 16√ó16 (optimal)
   - Preserves more information through the network
   - Less information loss during compression

#### Example of Learning Difference:

**Shallow network (3 stages):**
- "This pixel should be abnormal"
- "This pixel should be normal"
- Result: Sharp, jaggy transitions ‚ùå

**Deep network (5-6 stages):**
- "This region is transitioning from normal to abnormal"
- "The boundary should gradually blend based on surrounding context"
- "Maintain texture consistency while changing intensity"
- Result: Smooth, natural boundaries ‚úì

### Parameter Count vs Dataset Size

**Your Dataset:** 522 images (437 normal + 85 abnormal)

**Is 40-50M parameters too much?**

**No! Here's why:**

1. **Diffusion models are different:** Learning to denoise at 1000 timesteps
   - Effective samples = 522 √ó 1000 = 522,000 training examples

2. **EMA provides regularization:** Already implemented

3. **Data augmentation multiplies dataset:**
   - With augmentation: Effective dataset √ó 10-20
   - Actual capacity: 5,220-10,440 effective images

4. **Weight sharing:** U-Net symmetry means not all params are independent

**Current issue is likely UNDERFITTING (too simple), not overfitting!**

---

## Implementation Strategy

### Approach 1: Transform Resize (Recommended for Experimentation)

**Pros:**
- ‚úÖ Flexibility to test different resolutions quickly
- ‚úÖ Saves disk space (only store originals)
- ‚úÖ Easy to implement (one parameter change)
- ‚úÖ Can add data augmentation easily

**Cons:**
- ‚ùå Slower training (~10-20% overhead per epoch)
- ‚ùå CPU bottleneck during data loading

**Best for:** Experimentation phase, small datasets, testing multiple resolutions

### Approach 2: Preprocess Dataset (For Production)

**Pros:**
- ‚úÖ Faster training (no resize overhead)
- ‚úÖ Better GPU utilization
- ‚úÖ Reproducibility
- ‚úÖ Best for final training runs

**Cons:**
- ‚ùå More disk space (~10x for 128√ó128)
- ‚ùå Less flexible (need to reprocess for different resolutions)

**Best for:** Final training after resolution is decided

### Recommended Testing Strategy

**Phase 1: Quick Experimentation (Use Transform Resize)**
1. Test 128√ó128 with transform resize
2. Train for 100-200 epochs
3. Check boundary quality
4. Experiment with 96√ó96, 128√ó128, 160√ó160 if needed

**Phase 2: Production (Use Preprocessed Dataset)**
1. Finalize best resolution
2. Preprocess dataset at chosen resolution
3. Train full 1000 epochs with optimized pipeline
4. Add aggressive data augmentation

---

## Code Changes Required

### Minimal Changes for 128√ó128 Testing

```python
# In your training script (e.g., ddpm_train.py)

# 1. Update transform
transform = transforms.Compose([
    transforms.Resize((128, 128)),  # ‚Üê Just add this line
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

# 2. Update model creation
model = create_ddpm(
    image_size=128,  # ‚Üê Change from 40
    model_channels=64,  # ‚Üê Keep this
    channel_multipliers=(1, 2, 2, 4, 8),  # ‚Üê Add more stages
    num_res_blocks=2,  # ‚Üê Keep this
    use_attention=(False, False, False, True, True),  # ‚Üê Match length
    num_classes=2,
    num_timesteps=1000,
    beta_schedule="cosine",
    # ... rest of params
)

# 3. Optionally reduce batch_size if GPU memory limited
# batch_size = 4  # Instead of 8
```

### With Data Augmentation (Recommended)

```python
transform = transforms.Compose([
    transforms.Resize((140, 140)),
    transforms.RandomCrop((128, 128)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=10),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])
```

### Optional: Fix Upsampling (in ddpm.py UpBlock)

**Current:**
```python
if upsample:
    self.upsample_conv = nn.ConvTranspose2d(
        out_channels, out_channels, kernel_size=4, stride=2, padding=1
    )
```

**Better:**
```python
if upsample:
    self.upsample = nn.Sequential(
        nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
        nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
    )
```

---

## Memory and Training Considerations

| Resolution | Stages | Params  | Batch Size | GPU Memory | Training Time |
| ---------- | ------ | ------- | ---------- | ---------- | ------------- |
| 40√ó40      | 3      | ~15-20M | 8          | ~2GB       | 1x (baseline) |
| 128√ó128    | 5      | ~40-50M | 4-6        | ~6-8GB     | ~3-4x         |
| 256√ó256    | 6      | ~50-60M | 2-4        | ~10-14GB   | ~8-10x        |

**GPU Requirements:**
- 8GB VRAM: Safe for 128√ó128 with batch_size=4
- 16GB+ VRAM: Can do 256√ó256 with batch_size=2-4

---

## Expected Results

### Combined Impact of Improvements

| Change                                   | Expected Improvement       |
| ---------------------------------------- | -------------------------- |
| 128√ó128 resolution + deeper architecture | 70-80% boundary smoothness |
| Fix ConvTranspose2d upsampling           | +10-15% smoothness         |
| Data augmentation                        | +10-15% quality            |
| **Total Potential**                      | **~90-95% improvement**    |

### Training Convergence

Your current training shows excellent convergence:
- Epoch 1: loss=0.34 ‚Üí Epoch 352: loss=0.012
- Validation loss: 1.17 ‚Üí 0.053
- This indicates the model is learning well

**With deeper architecture:** Expect similar or better convergence with smoother outputs.

---

## Summary of Recommendations

### ‚úÖ DO THIS (In Order):

1. **Increase resolution to 128√ó128** (biggest impact)
2. **Use deeper architecture:** `channel_multipliers=(1, 2, 2, 4, 8)`
3. **Add data augmentation** (random crop, flip, rotation)
4. **Start with transform resize** for flexibility
5. **Fix upsampling** to bilinear interpolation + Conv2d
6. **Train 100-200 epochs** to test
7. **Once satisfied, preprocess dataset** for final training

### ‚ùå DON'T DO THIS:

1. Don't pursue Latent Diffusion Model (too complex, not beneficial)
2. Don't worry about "too many parameters" (you're likely underfitting)
3. Don't preprocess dataset yet (keep flexibility during testing)

### üéØ Expected Outcome:

- **Much smoother boundaries** on generated images
- **Better feature learning** from deeper network
- **Improved detail preservation** from higher resolution
- **More natural-looking abnormalities**

---

## Technical Notes

- Your `ddpm.py` architecture is already flexible - no code changes needed for different resolutions
- Just pass different parameters to `create_ddpm()`
- The architecture automatically adapts to any `channel_multipliers` tuple length
- EMA is already implemented - good for stability
- Training history shows good convergence - model is learning well
- Issue is architectural limitations, not training problems

---

## Conclusion

Your jaggy boundary issue is primarily due to:
1. **Low resolution (40√ó40)** - not enough pixels for smooth transitions
2. **Shallow architecture** - insufficient multi-scale feature learning
3. **ConvTranspose2d artifacts** - known to cause checkerboard patterns

**Solution is straightforward:** Increase to 128√ó128 with deeper architecture (5 stages). This is much simpler and more effective than pursuing Latent Diffusion Models.

The deeper network with more parameters will **help, not hurt** - your current model is likely underfitting, and the additional capacity will enable better feature learning and smoother boundaries.
